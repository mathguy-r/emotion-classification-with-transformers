{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathguy-r/sentiment-analysis-with-transformers/blob/temp/sentiment_analysis_distilbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Initialization"
      ],
      "metadata": {
        "id": "lEAFbFGPicHA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBYmICm_VvSV",
        "outputId": "2962f601-f06b-4755-91e4-cea8eb4603c0"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mc:\\Workplace\\sentiment-analysis-with-transformers\\sentiment_analysis_distilbert.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Workplace/sentiment-analysis-with-transformers/sentiment_analysis_distilbert.ipynb#ch0000000?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Workplace/sentiment-analysis-with-transformers/sentiment_analysis_distilbert.ipynb#ch0000000?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch --no-cache-dir\n",
        "!pip install tensorboard --no-cache-dir\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_7TYYPi_jdHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n"
      ],
      "metadata": {
        "id": "JEPaT5jXhxMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Handling"
      ],
      "metadata": {
        "id": "wKmWT2dch2Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data = pd.read_csv(obj['Body'])\n",
        "data = pd.read_csv('data_tagged_labelled_7k_2.csv')\n",
        "data.head(2)\n",
        "\n"
      ],
      "metadata": {
        "id": "SYH1fmzzh8FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "pJBU6UJ3iIvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## preprocessing \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# data['binary_labelled'] = data['label'].map({'Low':0,'Medium':1,'High':2})\n",
        "# print(data['binary_labelled'].value_counts())\n",
        "\n",
        "# X = data.loc[data.binary_labelled==0,'VALUE2'].tolist()[:8000] + \\\n",
        "#     data.loc[data.binary_labelled==1,'VALUE2'].tolist()[:8000] + \\\n",
        "#     data.loc[data.binary_labelled==2,'VALUE2'].tolist()[:8000]\n",
        "\n",
        "# y = data.loc[data.binary_labelled==0,'binary_labelled'].tolist()[:8000] + \\\n",
        "#     data.loc[data.binary_labelled==1,'binary_labelled'].tolist()[:8000] + \\\n",
        "#     data.loc[data.binary_labelled==2,'binary_labelled'].tolist()[:8000]\n",
        "\n",
        "X, y = data['text'].tolist(), data['label'].tolist()\n",
        "print(len(X),len(y))\n"
      ],
      "metadata": {
        "id": "yBcEcCxTiBKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "Yje7EHZqiM3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding, AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model_class, tokenizer_class, pretrained_weights = (AutoModelForSequenceClassification, AutoTokenizer, '.')\n",
        "\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights, do_lower_case=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "model = model_class.from_pretrained(pretrained_weights, num_labels=3)\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
        "\n",
        "test_df = pd.DataFrame({'text':Xtest,'label':ytest})\n",
        "test_df.to_csv('test_data.csv',index=False)\n",
        "\n",
        "train_dataset = Dataset.from_dict(dict(text=Xtrain, label=ytrain))\n",
        "test_dataset = Dataset.from_dict(dict(text=Xtest, label=ytest))\n",
        "\n",
        "preprocess_function = lambda x:tokenizer(x['text'], truncation=True)\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    }\n",
        "model_output_dir = \"./results\"\n",
        "model_saving_dir = \"./saved\"\n",
        "model_logging_dir = \"./logs\"\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '{model_logging_dir}'/runs\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_output_dir,\n",
        "    learning_rate=2e-6,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=model_logging_dir,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    report_to=\"tensorboard\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.evaluate()\n",
        "\n",
        "\n",
        "saving_path = model_saving_dir\n",
        "trainer.save_model(saving_path)\n",
        "tokenizer.save_pretrained(saving_path)\n",
        "\n",
        "#### Logging\n",
        "for obj in trainer.state.log_history:\n",
        "    print(obj)\n",
        "    \n",
        "# save train results\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "\n",
        "# save eval results\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)\n",
        "\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir '{model_logging_dir}'\n"
      ],
      "metadata": {
        "id": "ZveEjRbKi63a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "y9BiNMtLiUUx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r4metA7VvSZ"
      },
      "outputs": [],
      "source": [
        "### Load finetuned Model for prediction\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_class, tokenizer_class, pretrained_weights = (AutoModelForSequenceClassification, AutoTokenizer, 'saved')\n",
        "\n",
        "finetuned_tokenizer = tokenizer_class.from_pretrained(pretrained_weights, do_lower_case=True)\n",
        "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n",
        "finetuned_model = model_class.from_pretrained(pretrained_weights, num_labels=3)\n",
        "from transformers import TextClassificationPipeline\n",
        "text = \"75 thousands\"\n",
        "pipe = TextClassificationPipeline(model=finetuned_model, tokenizer=finetuned_tokenizer, return_all_scores=False)\n",
        "pipe(text)\n",
        "\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GmYyMIIuiZ8s"
      }
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "38740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "sentiment_analysis_distilbert.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}